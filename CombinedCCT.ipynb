{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import math\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_path(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        images.append(tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(os.path.join(path, file), target_size=(224, 224, 3))))\n",
    "        labels.append((label))\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "label_train = []\n",
    "label_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('dataset/train/A', 0)\n",
    "    \n",
    "train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('dataset/train/N', 1)\n",
    "    \n",
    "train += images\n",
    "label_train += labels\n",
    "\n",
    "images, labels = load_images_from_path('dataset/train/O', 2)\n",
    "    \n",
    "train += images\n",
    "label_train += labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('dataset/test/A', 0)\n",
    "    \n",
    "test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('dataset/test/N', 1)\n",
    "    \n",
    "test += images\n",
    "label_test += labels\n",
    "\n",
    "images, labels = load_images_from_path('dataset/test/O', 2)\n",
    "    \n",
    "test += images\n",
    "label_test += labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_norm = np.array(train) / 255\n",
    "test_norm = np.array(test) / 255\n",
    "\n",
    "\n",
    "label_train_encoded = to_categorical(label_train)\n",
    "label_test_encoded = to_categorical(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "input_shape = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_emb = True\n",
    "conv_layers = 2\n",
    "projection_dim = 128\n",
    "\n",
    "num_heads = 2\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 2\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCTTokenizer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=2,\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[64, 128],\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer.\n",
    "        self.conv_model = keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv2D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"valid\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # After passing the images through our mini-network the spatial dimensions\n",
    "        # are flattened to form sequences.\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        # Positional embeddings are optional in CCT. Here, we calculate\n",
    "        # the number of sequences and initialize an `Embedding` layer to\n",
    "        # compute the positional embeddings later.\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = tf.shape(dummy_outputs)[1]\n",
    "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
    "\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dim\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (tf.shape(x).shape[0] - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(scale=1.0 / 255),\n",
    "        layers.RandomCrop(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cct_model(\n",
    "    image_size=image_size,\n",
    "    input_shape=input_shape,\n",
    "    num_heads=num_heads,\n",
    "    projection_dim=projection_dim,\n",
    "    transformer_units=transformer_units,\n",
    "):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer()\n",
    "    encoded_patches = cct_tokenizer(augmented)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(weighted_representation)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"A\", \"N\", \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, num_folds=5, patience=16):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    histories = []\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for train_index, val_index in kf.split(train_norm):\n",
    "\n",
    "        fold += 1\n",
    "        print(f\"Training fold {fold}/{num_folds}...\")\n",
    "\n",
    "        x_train_fold, x_val_fold = train_norm[train_index], train_norm[val_index]\n",
    "        y_train_fold, y_val_fold = label_train_encoded[train_index], label_train_encoded[val_index]\n",
    "\n",
    "        optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\n",
    "                keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "                keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top-5-accuracy\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            x=x_train_fold,\n",
    "            y=y_train_fold,\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            validation_data=(x_val_fold, y_val_fold),\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        histories.append(history)\n",
    "\n",
    "        # Calculate evaluation metrics for this fold\n",
    "        y_pred_fold = model.predict(test_norm)\n",
    "        y_pred_labels_fold = np.argmax(y_pred_fold, axis=1)\n",
    "        y_true_labels_fold = np.argmax(label_test_encoded, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(y_true_labels_fold, y_pred_labels_fold)\n",
    "        precision = precision_score(y_true_labels_fold, y_pred_labels_fold, average='weighted')\n",
    "        recall = recall_score(y_true_labels_fold, y_pred_labels_fold, average='weighted')\n",
    "        f1 = f1_score(y_true_labels_fold, y_pred_labels_fold, average='weighted')\n",
    "        confusion = confusion_matrix(y_true_labels_fold, y_pred_labels_fold)\n",
    "\n",
    "        print(f\"Metrics for fold {fold}:\")\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 Score:\", f1)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion)\n",
    "\n",
    "        # Append true and predicted labels for this fold\n",
    "        all_true_labels.extend(y_true_labels_fold)\n",
    "        all_pred_labels.extend(y_pred_labels_fold)\n",
    "\n",
    "        # Generate and print a classification report for each class\n",
    "        class_report = classification_report(y_true_labels_fold, y_pred_labels_fold, target_names=class_names)\n",
    "        print(f\"Classification Report for Fold {fold}:\")\n",
    "        print(class_report)\n",
    "\n",
    "    # Calculate overall evaluation metrics after cross-validation\n",
    "    overall_accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    overall_precision = precision_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "    overall_recall = recall_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "    overall_f1 = f1_score(all_true_labels, all_pred_labels, average='weighted')\n",
    "    overall_confusion = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "\n",
    "    print(\"Overall Metrics After Cross-Validation:\")\n",
    "    print(\"Overall Accuracy:\", overall_accuracy)\n",
    "    print(\"Overall Precision:\", overall_precision)\n",
    "    print(\"Overall Recall:\", overall_recall)\n",
    "    print(\"Overall F1 Score:\", overall_f1)\n",
    "    print(\"Overall Confusion Matrix:\")\n",
    "    print(overall_confusion)\n",
    "\n",
    "    # Generate and print a classification report for each class after all folds\n",
    "    overall_class_report = classification_report(all_true_labels, all_pred_labels, target_names=class_names)\n",
    "    print(\"Overall Classification Report:\")\n",
    "    print(overall_class_report)\n",
    "\n",
    "    return histories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cct_model = create_cct_model()\n",
    "histories = run_experiment(cct_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
